{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9be53432",
   "metadata": {},
   "source": [
    "# Neural Network Approach\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Following our success with Gradient Boosting (XGBoost, LightGBM, CatBoost) in the previous notebooks, we now explore a Deep Learning approach. While tree-based models typically excel at tabular data, Neural Networks offer the potential to learn complex, non-linear representations.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Implement a Multi-Layer Perceptron (MLP) using PyTorch.\n",
    "2. Train the model on the same engineered features used in previous steps.\n",
    "3. Compare the performance of the Neural Network against our optimized Gradient Boosting models.\n",
    "4. Analyze the theoretical reasons behind the performance differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8302272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# For comparison\n",
    "import xgboost as xgb\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27be1006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('../data/training_data.csv')\n",
    "test_df = pd.read_csv('../data/test_data.csv')\n",
    "\n",
    "train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
    "test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n",
    "\n",
    "print(f\"Training: {train_df.shape}, Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same feature engineering as successful XGBoost\n",
    "def engineer_features(df, is_training=True):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Temporal\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
    "    df['dayofyear'] = df['timestamp'].dt.dayofyear\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    \n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    \n",
    "    # Wind features\n",
    "    df['wind_speed_squared'] = df['wind_speed_avg'] ** 2\n",
    "    df['wind_speed_cubed'] = df['wind_speed_avg'] ** 3\n",
    "    df['wind_speed_sqrt'] = np.sqrt(df['wind_speed_avg'])\n",
    "    \n",
    "    # Air density\n",
    "    df['temp_kelvin'] = df['outdoor_temp'] + 273.15\n",
    "    df['air_density_proxy'] = df['pressure'] / df['temp_kelvin']\n",
    "    df['wind_power_theoretical'] = df['air_density_proxy'] * df['wind_speed_cubed']\n",
    "    \n",
    "    # Directions\n",
    "    df['wind_nacelle_diff'] = np.abs(df['wind_angle'] - df['nacelle_angle'])\n",
    "    df['wind_nacelle_diff'] = df['wind_nacelle_diff'].apply(lambda x: min(x, 360-x) if x>180 else x)\n",
    "    df['nacelle_alignment'] = np.cos(np.radians(df['wind_nacelle_diff']))\n",
    "    \n",
    "    # Interactions\n",
    "    df['wind_temp_interaction'] = df['wind_speed_avg'] * df['outdoor_temp']\n",
    "    df['rotor_wind_interaction'] = df['rotor_angular_velocity'] * df['wind_speed_avg']\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_eng = engineer_features(train_df, True)\n",
    "test_eng = engineer_features(test_df, False)\n",
    "\n",
    "exclude = ['timestamp', 'active_power']\n",
    "features = sorted(list(set([c for c in train_eng.columns if c not in exclude]) & \n",
    "                       set([c for c in test_eng.columns if c != 'timestamp'])))\n",
    "\n",
    "X = train_eng[features].replace([np.inf, -np.inf], np.nan).fillna(train_eng[features].median())\n",
    "y = train_eng['active_power']\n",
    "X_test = test_eng[features].replace([np.inf, -np.inf], np.nan).fillna(X.median())\n",
    "\n",
    "print(f\"Features: {len(features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15481a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and scale\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train: {X_train_scaled.shape}, Val: {X_val_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f257535f",
   "metadata": {},
   "source": [
    "## 1. Neural Network Architecture\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) is designed to model the non-linear relationships in the wind turbine data. The architecture consists of three hidden layers with batch normalization and dropout for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178cba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindTurbineMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron for wind turbine power prediction\n",
    "    \n",
    "    Architecture:\n",
    "    - Input layer: n_features\n",
    "    - Hidden layer 1: 256 neurons + ReLU + Dropout\n",
    "    - Hidden layer 2: 128 neurons + ReLU + Dropout\n",
    "    - Hidden layer 3: 64 neurons + ReLU + Dropout\n",
    "    - Output layer: 1 neuron (regression)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, dropout=0.3):\n",
    "        super(WindTurbineMLP, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Linear(n_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Layer 2\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Layer 3\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Output\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "n_features = X_train_scaled.shape[1]\n",
    "model = WindTurbineMLP(n_features)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7160b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare PyTorch datasets\n",
    "train_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_train_scaled),\n",
    "    torch.FloatTensor(y_train.values).reshape(-1, 1)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_val_scaled),\n",
    "    torch.FloatTensor(y_val.values).reshape(-1, 1)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e7b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.L1Loss()  # MAE loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "print(f\"Training on: {device}\")\n",
    "print(f\"Loss function: MAE (L1Loss)\")\n",
    "print(f\"Optimizer: Adam with lr=0.001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9297a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(X_batch)\n",
    "    \n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            pred = model(X_batch)\n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            actuals.extend(y_batch.numpy())\n",
    "    \n",
    "    predictions = np.array(predictions).flatten()\n",
    "    actuals = np.array(actuals).flatten()\n",
    "    \n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    \n",
    "    return mae, rmse, r2, predictions\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffceec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_epochs = 50\n",
    "best_val_mae = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_maes = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_mae, val_rmse, val_r2, _ = validate(model, val_loader, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_maes.append(val_mae)\n",
    "    \n",
    "    scheduler.step(val_mae)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:3d}/{n_epochs} | Train Loss: {train_loss:7.4f} | Val MAE: {val_mae:7.4f} | Val RÂ²: {val_r2:.4f}\")\n",
    "    \n",
    "    if val_mae < best_val_mae:\n",
    "        best_val_mae = val_mae\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), '../models/mlp_best.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Best validation MAE: {best_val_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MAE Loss')\n",
    "ax1.set_title('Training Loss Over Time')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(val_maes, label='Validation MAE', color='orange')\n",
    "ax2.axhline(y=9.10, color='g', linestyle='--', label='XGBoost Ensemble (9.10)')\n",
    "ax2.axhline(y=9.15, color='b', linestyle='--', label='Optuna XGBoost (9.15)')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.set_title('Validation MAE vs Gradient Boosting Baselines')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/mlp_training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad76188",
   "metadata": {},
   "source": [
    "## 2. Performance Comparison\n",
    "\n",
    "The performance of the neural network is compared against the XGBoost baseline to evaluate the suitability of deep learning for this specific tabular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f00e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train simple XGBoost for comparison\n",
    "print(\"Training XGBoost baseline for comparison...\")\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "xgb_pred = xgb_model.predict(X_val_scaled)\n",
    "xgb_mae = mean_absolute_error(y_val, xgb_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Neural Network (MLP):        {best_val_mae:.4f} MAE\")\n",
    "print(f\"XGBoost (default params):    {xgb_mae:.4f} MAE\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "if best_val_mae > xgb_mae:\n",
    "    diff_pct = ((best_val_mae - xgb_mae) / xgb_mae) * 100\n",
    "    print(f\"MLP is {diff_pct:.1f}% worse than XGBoost\")\n",
    "else:\n",
    "    print(\"MLP performed better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a84be",
   "metadata": {},
   "source": [
    "## 3. Discussion on Neural Network Performance\n",
    "\n",
    "### Theoretical Considerations\n",
    "\n",
    "1. **Inductive Bias**: Neural networks typically assume smooth, continuous functions, whereas wind turbine data often exhibits sharp transitions due to operational states (e.g., cut-in, rated power, cut-out). Tree-based models are naturally better suited for such discrete decision boundaries.\n",
    "2. **Sample Efficiency**: Deep learning models generally require larger datasets to generalize effectively compared to gradient boosting machines, which perform well on medium-sized tabular datasets.\n",
    "3. **Feature Interactions**: While neural networks can learn feature interactions, tree-based models explicitly split on features, often capturing interactions more efficiently in tabular domains.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "For this specific problem and dataset size, gradient boosting methods (XGBoost, LightGBM, CatBoost) demonstrate superior performance and efficiency compared to the MLP architecture implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a40c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate MLP predictions for test set\n",
    "model.load_state_dict(torch.load('../models/mlp_best.pt'))\n",
    "model.eval()\n",
    "\n",
    "test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "with torch.no_grad():\n",
    "    mlp_test_pred = model(test_tensor).cpu().numpy().flatten()\n",
    "\n",
    "mlp_test_pred = np.clip(mlp_test_pred, 0, 2100)\n",
    "\n",
    "mlp_submission = pd.DataFrame({\n",
    "    'id': range(len(mlp_test_pred)),\n",
    "    'active_power': mlp_test_pred\n",
    "})\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "mlp_submission.to_csv(f'../results/mlp_submission_{timestamp}.csv', index=False)\n",
    "\n",
    "print(f\"MLP submission saved.\")\n",
    "print(f\"Predictions - Min: {mlp_test_pred.min():.2f}, Max: {mlp_test_pred.max():.2f}, Mean: {mlp_test_pred.mean():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
