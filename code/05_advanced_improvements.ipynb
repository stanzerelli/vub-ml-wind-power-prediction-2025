{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8626998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(f\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc777a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('../data/training_data.csv')\n",
    "test_df = pd.read_csv('../data/test_data.csv')\n",
    "\n",
    "train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
    "test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n",
    "\n",
    "print(f\"Training: {train_df.shape}, Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004dc794",
   "metadata": {},
   "source": [
    "## 1. Enhanced Feature Engineering\n",
    "\n",
    "Building upon the baseline features, we add:\n",
    "- Power coefficient approximation (Cp)\n",
    "- Tip speed ratio (TSR)\n",
    "- Turbulence intensity proxies\n",
    "- Higher-order polynomial features\n",
    "- More sophisticated interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a555d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_feature_engineering(df, is_training=True):\n",
    "    \"\"\"\n",
    "    Enhanced feature engineering with physics-based and advanced features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ========== Basic Temporal Features ==========\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
    "    df['dayofyear'] = df['timestamp'].dt.dayofyear\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    \n",
    "    # Time of day categories (better than hour alone)\n",
    "    df['time_of_day'] = pd.cut(df['hour'], bins=[0, 6, 12, 18, 24], \n",
    "                                labels=['night', 'morning', 'afternoon', 'evening'], include_lowest=True)\n",
    "    df['is_night'] = (df['time_of_day'] == 'night').astype(int)\n",
    "    df['is_morning'] = (df['time_of_day'] == 'morning').astype(int)\n",
    "    df['is_afternoon'] = (df['time_of_day'] == 'afternoon').astype(int)\n",
    "    \n",
    "    # Cyclic encoding\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['dayofyear_sin'] = np.sin(2 * np.pi * df['dayofyear'] / 365)\n",
    "    df['dayofyear_cos'] = np.cos(2 * np.pi * df['dayofyear'] / 365)\n",
    "    \n",
    "    # ========== Advanced Wind Speed Features ==========\n",
    "    # Polynomial features (up to 4th order)\n",
    "    df['wind_speed_squared'] = df['wind_speed_avg'] ** 2\n",
    "    df['wind_speed_cubed'] = df['wind_speed_avg'] ** 3\n",
    "    df['wind_speed_4th'] = df['wind_speed_avg'] ** 4\n",
    "    df['wind_speed_sqrt'] = np.sqrt(df['wind_speed_avg'])\n",
    "    df['wind_speed_log'] = np.log1p(df['wind_speed_avg'])  # log(1+x) to handle zeros\n",
    "    \n",
    "    # Wind speed bins (categorical feature as indicators)\n",
    "    df['wind_very_low'] = (df['wind_speed_avg'] < 4).astype(int)\n",
    "    df['wind_low'] = ((df['wind_speed_avg'] >= 4) & (df['wind_speed_avg'] < 8)).astype(int)\n",
    "    df['wind_medium'] = ((df['wind_speed_avg'] >= 8) & (df['wind_speed_avg'] < 12)).astype(int)\n",
    "    df['wind_high'] = ((df['wind_speed_avg'] >= 12) & (df['wind_speed_avg'] < 16)).astype(int)\n",
    "    df['wind_very_high'] = (df['wind_speed_avg'] >= 16).astype(int)\n",
    "    \n",
    "    # Sensor agreement metrics\n",
    "    df['wind_speed_diff'] = np.abs(df['wind_speed1'] - df['wind_speed2'])\n",
    "    df['wind_speed_ratio'] = df['wind_speed1'] / (df['wind_speed2'] + 0.001)\n",
    "    df['wind_speed_max'] = df[['wind_speed1', 'wind_speed2']].max(axis=1)\n",
    "    df['wind_speed_min'] = df[['wind_speed1', 'wind_speed2']].min(axis=1)\n",
    "    df['wind_speed_cv'] = df['wind_speed_diff'] / (df['wind_speed_avg'] + 0.001)  # Coefficient of variation\n",
    "    \n",
    "    # Turbulence intensity proxy (based on sensor disagreement)\n",
    "    df['turbulence_proxy'] = df['wind_speed_diff'] / (df['wind_speed_avg'] + 0.001)\n",
    "    \n",
    "    # ========== Air Density & Theoretical Power ==========\n",
    "    df['temp_kelvin'] = df['outdoor_temp'] + 273.15\n",
    "    df['air_density_proxy'] = df['pressure'] / df['temp_kelvin']\n",
    "    \n",
    "    # Theoretical wind power (P = 0.5 * ρ * A * v³)\n",
    "    # Assuming swept area A is constant, we focus on ρ * v³\n",
    "    df['wind_power_theoretical'] = df['air_density_proxy'] * df['wind_speed_cubed']\n",
    "    df['wind_power_theoretical_normalized'] = df['wind_power_theoretical'] / (df['wind_power_theoretical'].mean() + 0.001)\n",
    "    \n",
    "    # ========== Rotor & Tip Speed Ratio ==========\n",
    "    df['rotor_angular_velocity_squared'] = df['rotor_angular_velocity'] ** 2\n",
    "    \n",
    "    # Tip Speed Ratio (TSR) = (ω * R) / v\n",
    "    # Assuming rotor radius R is constant (typically 40-50m for 2MW turbines)\n",
    "    # We can use relative TSR without knowing R\n",
    "    df['tsr_proxy'] = df['rotor_angular_velocity'] / (df['wind_speed_avg'] + 0.001)\n",
    "    df['tsr_proxy_squared'] = df['tsr_proxy'] ** 2\n",
    "    \n",
    "    # Power coefficient proxy (Cp) - actual power divided by theoretical\n",
    "    # Only meaningful during training when we have actual power\n",
    "    if is_training and 'active_power' in df.columns:\n",
    "        df['cp_proxy'] = df['active_power'] / (df['wind_power_theoretical'] + 0.001)\n",
    "        df['cp_proxy'] = df['cp_proxy'].clip(0, 0.593)  # Betz limit\n",
    "    \n",
    "    # ========== Direction Alignment Features ==========\n",
    "    # Wind-nacelle alignment (how well turbine faces the wind)\n",
    "    df['wind_nacelle_diff'] = np.abs(df['wind_angle'] - df['nacelle_angle'])\n",
    "    df['wind_nacelle_diff'] = df['wind_nacelle_diff'].apply(lambda x: min(x, 360 - x) if x > 180 else x)\n",
    "    df['wind_vane_diff'] = np.abs(df['wind_angle'] - df['vane_angle'])\n",
    "    df['wind_vane_diff'] = df['wind_vane_diff'].apply(lambda x: min(x, 360 - x) if x > 180 else x)\n",
    "    \n",
    "    # Cosine of misalignment (1 = perfect, 0 = perpendicular, -1 = opposite)\n",
    "    df['nacelle_alignment'] = np.cos(np.radians(df['wind_nacelle_diff']))\n",
    "    df['vane_alignment'] = np.cos(np.radians(df['wind_vane_diff']))\n",
    "    \n",
    "    # Yaw error indicators\n",
    "    df['large_yaw_error'] = (df['wind_nacelle_diff'] > 15).astype(int)\n",
    "    df['moderate_yaw_error'] = ((df['wind_nacelle_diff'] > 5) & (df['wind_nacelle_diff'] <= 15)).astype(int)\n",
    "    \n",
    "    # ========== Pitch Angle & Operational States ==========\n",
    "    df['pitch_angle_squared'] = df['pitch_angle'] ** 2\n",
    "    df['pitch_angle_cubed'] = df['pitch_angle'] ** 3\n",
    "    \n",
    "    # Operational state indicators\n",
    "    df['is_shutdown'] = (df['pitch_angle'] > 40).astype(int)\n",
    "    df['is_feathering'] = ((df['pitch_angle'] > 20) & (df['pitch_angle'] <= 40)).astype(int)\n",
    "    df['is_optimal_pitch'] = (df['pitch_angle'] <= 5).astype(int)\n",
    "    \n",
    "    # Pitch rate of change (if we have sequential data)\n",
    "    if is_training:\n",
    "        df['pitch_change'] = df['pitch_angle'].diff().fillna(0)\n",
    "        df['pitch_change_abs'] = np.abs(df['pitch_change'])\n",
    "    \n",
    "    # ========== Temperature Features ==========\n",
    "    df['temp_diff'] = df['outdoor_temp'] - df['nacelle_temp']\n",
    "    df['weather_outdoor_temp_diff'] = df['weather_temp'] - df['outdoor_temp']\n",
    "    df['temp_diff_squared'] = df['temp_diff'] ** 2\n",
    "    \n",
    "    # Temperature categories\n",
    "    df['is_cold'] = (df['outdoor_temp'] < 0).astype(int)\n",
    "    df['is_hot'] = (df['outdoor_temp'] > 25).astype(int)\n",
    "    \n",
    "    # ========== Weather Condition Features ==========\n",
    "    df['weather_wind_diff'] = np.abs(df['weather_wind_speed'] - df['wind_speed_avg'])\n",
    "    df['has_rain'] = (df['rain_1h'] > 0).astype(int)\n",
    "    df['has_snow'] = (df['snow_1h'] > 0).astype(int)\n",
    "    df['has_precipitation'] = ((df['rain_1h'] > 0) | (df['snow_1h'] > 0)).astype(int)\n",
    "    \n",
    "    # Rain/snow intensity\n",
    "    df['rain_log'] = np.log1p(df['rain_1h'])\n",
    "    df['snow_log'] = np.log1p(df['snow_1h'])\n",
    "    \n",
    "    # Humidity effects\n",
    "    df['humidity_high'] = (df['humidity'] > 80).astype(int)\n",
    "    df['humidity_low'] = (df['humidity'] < 40).astype(int)\n",
    "    \n",
    "    # ========== Advanced Interaction Features ==========\n",
    "    # Wind power affected by alignment\n",
    "    df['wind_power_aligned'] = df['wind_power_theoretical'] * df['nacelle_alignment']\n",
    "    \n",
    "    # Wind-temperature interactions (air density effects)\n",
    "    df['wind_temp_interaction'] = df['wind_speed_avg'] * df['outdoor_temp']\n",
    "    df['wind_pressure_interaction'] = df['wind_speed_avg'] * df['pressure']\n",
    "    df['wind_density_interaction'] = df['wind_speed_avg'] * df['air_density_proxy']\n",
    "    \n",
    "    # Rotor-wind interactions\n",
    "    df['rotor_wind_interaction'] = df['rotor_angular_velocity'] * df['wind_speed_avg']\n",
    "    df['rotor_wind_squared'] = df['rotor_angular_velocity'] * df['wind_speed_squared']\n",
    "    \n",
    "    # Pitch-wind interactions (control system response)\n",
    "    df['pitch_wind_interaction'] = df['pitch_angle'] * df['wind_speed_avg']\n",
    "    df['pitch_rotor_interaction'] = df['pitch_angle'] * df['rotor_angular_velocity']\n",
    "    \n",
    "    # Complex interaction: wind power with pitch control\n",
    "    df['controlled_power_proxy'] = df['wind_power_theoretical'] * np.cos(np.radians(df['pitch_angle']))\n",
    "    \n",
    "    # ========== Statistical Aggregations (Training only) ==========\n",
    "    if is_training:\n",
    "        # Rolling statistics (short-term trends)\n",
    "        for window in [3, 6, 12]:  # 30 min, 1 hour, 2 hours\n",
    "            df[f'wind_rolling_mean_{window}'] = df['wind_speed_avg'].rolling(window=window, min_periods=1).mean()\n",
    "            df[f'wind_rolling_std_{window}'] = df['wind_speed_avg'].rolling(window=window, min_periods=1).std()\n",
    "            df[f'wind_rolling_max_{window}'] = df['wind_speed_avg'].rolling(window=window, min_periods=1).max()\n",
    "            df[f'wind_rolling_min_{window}'] = df['wind_speed_avg'].rolling(window=window, min_periods=1).min()\n",
    "            \n",
    "            # Rotor rolling stats\n",
    "            df[f'rotor_rolling_mean_{window}'] = df['rotor_angular_velocity'].rolling(window=window, min_periods=1).mean()\n",
    "            \n",
    "            # Power rolling stats (if available)\n",
    "            if 'active_power' in df.columns:\n",
    "                df[f'power_rolling_mean_{window}'] = df['active_power'].rolling(window=window, min_periods=1).mean()\n",
    "                df[f'power_rolling_std_{window}'] = df['active_power'].rolling(window=window, min_periods=1).std()\n",
    "        \n",
    "        # Lag features (previous timestep)\n",
    "        for lag in [1, 2, 3]:\n",
    "            df[f'wind_lag_{lag}'] = df['wind_speed_avg'].shift(lag).fillna(df['wind_speed_avg'].mean())\n",
    "            df[f'pitch_lag_{lag}'] = df['pitch_angle'].shift(lag).fillna(df['pitch_angle'].mean())\n",
    "    \n",
    "    # ========== Manufacturer Power Curve Features ==========\n",
    "    # Theoretical power curve (simplified Vestas V90-2.0 MW)\n",
    "    def theoretical_power(wind_speed):\n",
    "        if wind_speed < 4:  # Cut-in speed\n",
    "            return 0\n",
    "        elif wind_speed < 12:  # Ramp-up region (cubic relationship)\n",
    "            return 2050 * ((wind_speed - 4) / (12 - 4)) ** 3\n",
    "        elif wind_speed < 25:  # Rated power region\n",
    "            return 2050\n",
    "        else:  # Cut-out\n",
    "            return 0\n",
    "    \n",
    "    df['theoretical_power_curve'] = df['wind_speed_avg'].apply(theoretical_power)\n",
    "    \n",
    "    # Deviation from theoretical curve (training only)\n",
    "    if is_training and 'active_power' in df.columns:\n",
    "        df['power_curve_deviation'] = df['active_power'] - df['theoretical_power_curve']\n",
    "        df['power_curve_ratio'] = df['active_power'] / (df['theoretical_power_curve'] + 0.001)\n",
    "    \n",
    "    # Drop intermediate columns we don't want as features\n",
    "    if 'time_of_day' in df.columns:\n",
    "        df = df.drop('time_of_day', axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Enhanced feature engineering function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c0070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply enhanced feature engineering\n",
    "print(\"Engineering features...\")\n",
    "train_eng = enhanced_feature_engineering(train_df, is_training=True)\n",
    "test_eng = enhanced_feature_engineering(test_df, is_training=False)\n",
    "\n",
    "print(f\"\\nOriginal features: {len(train_df.columns)}\")\n",
    "print(f\"Engineered features: {len(train_eng.columns)}\")\n",
    "print(f\"New features created: {len(train_eng.columns) - len(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa9cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "exclude_cols = ['timestamp', 'active_power', 'reactive_power', 'cp_proxy', 'power_curve_deviation', 'power_curve_ratio']\n",
    "train_features = [col for col in train_eng.columns if col not in exclude_cols]\n",
    "test_features = [col for col in test_eng.columns if col not in ['timestamp']]\n",
    "common_features = sorted(list(set(train_features) & set(test_features)))\n",
    "\n",
    "print(f\"\\nCommon features for modeling: {len(common_features)}\")\n",
    "\n",
    "X = train_eng[common_features].replace([np.inf, -np.inf], np.nan).fillna(train_eng[common_features].median())\n",
    "y = train_eng['active_power']\n",
    "X_test = test_eng[common_features].replace([np.inf, -np.inf], np.nan).fillna(X.median())\n",
    "\n",
    "print(f\"Training samples: {len(X):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f6aff",
   "metadata": {},
   "source": [
    "## 2. Feature Selection\n",
    "\n",
    "Not all features improve performance. We use mutual information to identify the most predictive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a528fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance using mutual information\n",
    "print(\"Computing mutual information scores...\")\n",
    "mi_scores = mutual_info_regression(X, y, random_state=RANDOM_STATE, n_neighbors=5)\n",
    "mi_scores = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\nTop 30 features by mutual information:\")\n",
    "print(mi_scores.head(30))\n",
    "\n",
    "# Select top features (keep top 80% by MI score)\n",
    "threshold = mi_scores.quantile(0.20)  # Drop bottom 20%\n",
    "selected_features = mi_scores[mi_scores > threshold].index.tolist()\n",
    "\n",
    "print(f\"\\nFeatures selected: {len(selected_features)} out of {len(common_features)}\")\n",
    "print(f\"Features dropped: {len(common_features) - len(selected_features)}\")\n",
    "\n",
    "# Update datasets\n",
    "X = X[selected_features]\n",
    "X_test = X_test[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dcf8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and scale\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train: {X_train_scaled.shape}, Val: {X_val_scaled.shape}, Test: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad07a49d",
   "metadata": {},
   "source": [
    "## 3. State-Based Modeling\n",
    "\n",
    "Train separate models for different operational states to capture their unique dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b36a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define operational states\n",
    "def get_operational_state(row):\n",
    "    \"\"\"Categorize operational state based on pitch angle and wind speed\"\"\"\n",
    "    if row['pitch_angle'] > 40:\n",
    "        return 'shutdown'\n",
    "    elif row['wind_speed_avg'] < 4:\n",
    "        return 'cut_in'\n",
    "    elif row['wind_speed_avg'] >= 12:\n",
    "        return 'rated'\n",
    "    else:\n",
    "        return 'normal'\n",
    "\n",
    "# Apply to training data\n",
    "train_eng['state'] = train_eng.apply(get_operational_state, axis=1)\n",
    "test_eng['state'] = test_eng.apply(get_operational_state, axis=1)\n",
    "\n",
    "print(\"Training data state distribution:\")\n",
    "print(train_eng['state'].value_counts())\n",
    "print(f\"\\nTest data state distribution:\")\n",
    "print(test_eng['state'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71706063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create state-specific datasets\n",
    "states = ['shutdown', 'cut_in', 'normal', 'rated']\n",
    "state_models = {}\n",
    "state_predictions = {}\n",
    "\n",
    "for state in states:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training model for state: {state}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get state-specific data\n",
    "    train_state_mask = train_eng['state'] == state\n",
    "    val_state_mask = train_eng.iloc[split_idx:]['state'] == state\n",
    "    test_state_mask = test_eng['state'] == state\n",
    "    \n",
    "    train_state_idx = train_state_mask.iloc[:split_idx]\n",
    "    \n",
    "    X_train_state = X_train_scaled[train_state_idx]\n",
    "    y_train_state = y_train[train_state_idx]\n",
    "    X_val_state = X_val_scaled[val_state_mask]\n",
    "    y_val_state = y_val.iloc[split_idx:][val_state_mask]\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train_state)}\")\n",
    "    print(f\"Validation samples: {len(X_val_state)}\")\n",
    "    \n",
    "    if len(X_train_state) < 100:  # Skip if too few samples\n",
    "        print(f\"Skipping {state} - insufficient data\")\n",
    "        continue\n",
    "    \n",
    "    # Train XGBoost for this state\n",
    "    if state == 'shutdown':\n",
    "        # Shutdown state: predict near-zero\n",
    "        model = xgb.XGBRegressor(n_estimators=50, max_depth=3, learning_rate=0.05, random_state=RANDOM_STATE)\n",
    "    elif state == 'cut_in':\n",
    "        # Cut-in: low wind, careful modeling\n",
    "        model = xgb.XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.05, random_state=RANDOM_STATE)\n",
    "    elif state == 'rated':\n",
    "        # Rated: predict around 2050 kW\n",
    "        model = xgb.XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.05, random_state=RANDOM_STATE)\n",
    "    else:  # normal\n",
    "        # Normal operation: most complex region\n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=200,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "    \n",
    "    model.fit(X_train_state, y_train_state)\n",
    "    \n",
    "    # Evaluate\n",
    "    if len(X_val_state) > 0:\n",
    "        val_pred = model.predict(X_val_state)\n",
    "        mae = mean_absolute_error(y_val_state, val_pred)\n",
    "        print(f\"Validation MAE: {mae:.4f}\")\n",
    "    \n",
    "    state_models[state] = model\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"State-specific models trained: {len(state_models)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ace942",
   "metadata": {},
   "source": [
    "## 4. Unified Global Model\n",
    "\n",
    "Train a global model on all data as the primary predictor, with state models as fallback/ensemble members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1b92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train global XGBoost model\n",
    "print(\"Training global XGBoost model...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.1,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    eval_set=[(X_val_scaled, y_val)],\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "xgb_pred = xgb_model.predict(X_val_scaled)\n",
    "xgb_mae = mean_absolute_error(y_val, xgb_pred)\n",
    "print(f\"\\nGlobal XGBoost MAE: {xgb_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5566aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train global LightGBM model\n",
    "print(\"\\nTraining global LightGBM model...\")\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_samples=20,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgb_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    eval_set=[(X_val_scaled, y_val)],\n",
    "    callbacks=[lgb.log_evaluation(50)]\n",
    ")\n",
    "\n",
    "lgb_pred = lgb_model.predict(X_val_scaled)\n",
    "lgb_mae = mean_absolute_error(y_val, lgb_pred)\n",
    "print(f\"\\nGlobal LightGBM MAE: {lgb_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f41a8ad",
   "metadata": {},
   "source": [
    "## 5. Intelligent Ensemble\n",
    "\n",
    "Combine global models with conditional weighting based on operational state and confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ede53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble with state-aware weighting\n",
    "def smart_ensemble(xgb_pred, lgb_pred, states, base_weights=(0.6, 0.4)):\n",
    "    \"\"\"\n",
    "    Intelligent ensemble that adjusts weights based on operational state\n",
    "    \"\"\"\n",
    "    ensemble_pred = np.zeros_like(xgb_pred)\n",
    "    \n",
    "    for i in range(len(xgb_pred)):\n",
    "        state = states.iloc[i]\n",
    "        \n",
    "        # Adjust weights based on state\n",
    "        if state == 'shutdown':\n",
    "            # In shutdown, heavily weight toward XGBoost (seems more stable)\n",
    "            w_xgb, w_lgb = 0.8, 0.2\n",
    "        elif state == 'rated':\n",
    "            # In rated region, equal weighting\n",
    "            w_xgb, w_lgb = 0.5, 0.5\n",
    "        elif state == 'cut_in':\n",
    "            # Low wind is tricky, favor XGBoost\n",
    "            w_xgb, w_lgb = 0.7, 0.3\n",
    "        else:  # normal\n",
    "            # Normal operation: use base weights\n",
    "            w_xgb, w_lgb = base_weights\n",
    "        \n",
    "        ensemble_pred[i] = w_xgb * xgb_pred[i] + w_lgb * lgb_pred[i]\n",
    "    \n",
    "    return ensemble_pred\n",
    "\n",
    "# Apply smart ensemble\n",
    "val_states = train_eng.iloc[split_idx:]['state']\n",
    "ensemble_pred = smart_ensemble(xgb_pred, lgb_pred, val_states)\n",
    "ensemble_mae = mean_absolute_error(y_val, ensemble_pred)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ENSEMBLE PERFORMANCE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"XGBoost MAE:      {xgb_mae:.4f}\")\n",
    "print(f\"LightGBM MAE:     {lgb_mae:.4f}\")\n",
    "print(f\"Smart Ensemble:   {ensemble_mae:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "best_mae = min(xgb_mae, lgb_mae, ensemble_mae)\n",
    "print(f\"\\nBest validation MAE: {best_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179454b7",
   "metadata": {},
   "source": [
    "## 6. Post-Processing & Physics Constraints\n",
    "\n",
    "Apply domain knowledge to refine predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d10f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_physics_constraints(predictions, features_df):\n",
    "    \"\"\"\n",
    "    Apply physics-based constraints to predictions\n",
    "    \"\"\"\n",
    "    predictions = predictions.copy()\n",
    "    \n",
    "    # 1. Hard constraints\n",
    "    predictions = np.clip(predictions, 0, 2100)  # Physical limits\n",
    "    \n",
    "    # 2. Shutdown state: force near-zero\n",
    "    shutdown_mask = features_df['pitch_angle'] > 40\n",
    "    predictions[shutdown_mask] = np.minimum(predictions[shutdown_mask], 50)\n",
    "    \n",
    "    # 3. Cut-in speed: no power below 3 m/s\n",
    "    cut_in_mask = features_df['wind_speed_avg'] < 3\n",
    "    predictions[cut_in_mask] = 0\n",
    "    \n",
    "    # 4. Cut-out speed: no power above 25 m/s\n",
    "    cut_out_mask = features_df['wind_speed_avg'] > 25\n",
    "    predictions[cut_out_mask] = 0\n",
    "    \n",
    "    # 5. Rated region: cap at rated power with some variance\n",
    "    rated_mask = features_df['wind_speed_avg'] >= 12\n",
    "    predictions[rated_mask] = np.clip(predictions[rated_mask], 1800, 2100)\n",
    "    \n",
    "    # 6. Low wind region (3-5 m/s): limit max power\n",
    "    low_wind_mask = (features_df['wind_speed_avg'] >= 3) & (features_df['wind_speed_avg'] < 5)\n",
    "    predictions[low_wind_mask] = np.minimum(predictions[low_wind_mask], 400)\n",
    "    \n",
    "    # 7. Negative to zero\n",
    "    predictions = np.maximum(predictions, 0)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "print(\"Physics constraints function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a261fb",
   "metadata": {},
   "source": [
    "## 7. Generate Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879fd701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply best model to test data\n",
    "print(\"Generating test predictions...\")\n",
    "\n",
    "# Get predictions from all models\n",
    "xgb_test = xgb_model.predict(X_test_scaled)\n",
    "lgb_test = lgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Apply smart ensemble\n",
    "test_states = test_eng['state']\n",
    "ensemble_test = smart_ensemble(xgb_test, lgb_test, test_states)\n",
    "\n",
    "# Apply physics constraints\n",
    "final_predictions = apply_physics_constraints(ensemble_test, test_eng)\n",
    "\n",
    "print(f\"\\nPrediction statistics:\")\n",
    "print(f\"Min: {final_predictions.min():.2f}\")\n",
    "print(f\"Max: {final_predictions.max():.2f}\")\n",
    "print(f\"Mean: {final_predictions.mean():.2f}\")\n",
    "print(f\"Std: {final_predictions.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea8ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(final_predictions)),\n",
    "    'active_power': final_predictions\n",
    "})\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "filename = f'../results/v5_advanced_submission_{timestamp}.csv'\n",
    "submission.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"\\nSubmission saved: {filename}\")\n",
    "print(f\"\\nEstimated validation MAE: {best_mae:.4f}\")\n",
    "print(f\"Target: < 9.13 (current best)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6371d5c",
   "metadata": {},
   "source": [
    "## 8. Additional Strategies to Try\n",
    "\n",
    "If the above doesn't break 9.13, consider:\n",
    "\n",
    "### A. Pseudo-labeling\n",
    "- Use test predictions with high confidence to augment training data\n",
    "- Retrain with combined dataset\n",
    "\n",
    "### B. Cross-validation ensemble\n",
    "- Train multiple models on different CV folds\n",
    "- Average their predictions\n",
    "\n",
    "### C. Feature engineering v2\n",
    "- Fourier features for time series patterns\n",
    "- Distance to nearest training example\n",
    "- Clustering-based features\n",
    "\n",
    "### D. Hyperparameter optimization on new features\n",
    "- Re-run Optuna with enhanced feature set\n",
    "- May find better parameters\n",
    "\n",
    "### E. Quantile regression\n",
    "- Predict uncertainty bounds\n",
    "- Adjust predictions based on confidence\n",
    "\n",
    "### F. External data\n",
    "- If allowed: weather patterns, seasonal trends\n",
    "- Historical turbine performance data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
