{
 "cells": [
  {
   "cell_type": "code",
   "id": "4ce1775f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:21:28.330369Z",
     "start_time": "2025-11-21T17:21:28.140201Z"
    }
   },
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "print(f\"Libraries loaded successfully\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "d19502ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:21:28.792515Z",
     "start_time": "2025-11-21T17:21:28.334338Z"
    }
   },
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('../data/training_data.csv')\n",
    "test_df = pd.read_csv('../data/test_data.csv')\n",
    "\n",
    "train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
    "test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n",
    "\n",
    "print(f\"Training: {train_df.shape}, Test: {test_df.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (208910, 21), Test: (52228, 20)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "0e9a1bee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:21:28.813296Z",
     "start_time": "2025-11-21T17:21:28.805981Z"
    }
   },
   "source": [
    "# Feature engineering (V1 working baseline)\n",
    "def engineer_features(df, is_training=True):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Temporal\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
    "    df['dayofyear'] = df['timestamp'].dt.dayofyear\n",
    "    df['quarter'] = df['timestamp'].dt.quarter\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    \n",
    "    # Cyclic encoding\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['dayofyear_sin'] = np.sin(2 * np.pi * df['dayofyear'] / 365)\n",
    "    df['dayofyear_cos'] = np.cos(2 * np.pi * df['dayofyear'] / 365)\n",
    "    \n",
    "    # Wind speed features\n",
    "    df['wind_speed_squared'] = df['wind_speed_avg'] ** 2\n",
    "    df['wind_speed_cubed'] = df['wind_speed_avg'] ** 3\n",
    "    df['wind_speed_sqrt'] = np.sqrt(df['wind_speed_avg'])\n",
    "    df['wind_speed_diff'] = np.abs(df['wind_speed1'] - df['wind_speed2'])\n",
    "    df['wind_speed_ratio'] = df['wind_speed1'] / (df['wind_speed2'] + 0.001)\n",
    "    df['wind_speed_max'] = df[['wind_speed1', 'wind_speed2']].max(axis=1)\n",
    "    df['wind_speed_min'] = df[['wind_speed1', 'wind_speed2']].min(axis=1)\n",
    "    \n",
    "    # Air density\n",
    "    df['temp_kelvin'] = df['outdoor_temp'] + 273.15\n",
    "    df['air_density_proxy'] = df['pressure'] / df['temp_kelvin']\n",
    "    df['wind_power_theoretical'] = df['air_density_proxy'] * df['wind_speed_cubed']\n",
    "    \n",
    "    # Direction features\n",
    "    df['wind_nacelle_diff'] = np.abs(df['wind_angle'] - df['nacelle_angle'])\n",
    "    df['wind_nacelle_diff'] = df['wind_nacelle_diff'].apply(lambda x: min(x, 360 - x) if x > 180 else x)\n",
    "    df['wind_vane_diff'] = np.abs(df['wind_angle'] - df['vane_angle'])\n",
    "    df['wind_vane_diff'] = df['wind_vane_diff'].apply(lambda x: min(x, 360 - x) if x > 180 else x)\n",
    "    df['nacelle_alignment'] = np.cos(np.radians(df['wind_nacelle_diff']))\n",
    "    \n",
    "    # Temperature\n",
    "    df['temp_diff'] = df['outdoor_temp'] - df['nacelle_temp']\n",
    "    df['weather_outdoor_temp_diff'] = df['weather_temp'] - df['outdoor_temp']\n",
    "    \n",
    "    # Rotor\n",
    "    df['rotor_angular_velocity_squared'] = df['rotor_angular_velocity'] ** 2\n",
    "    \n",
    "    # Pitch angle\n",
    "    df['is_shutdown'] = (df['pitch_angle'] > 40).astype(int)\n",
    "    df['pitch_angle_squared'] = df['pitch_angle'] ** 2\n",
    "    \n",
    "    # Weather\n",
    "    df['weather_wind_diff'] = np.abs(df['weather_wind_speed'] - df['wind_speed_avg'])\n",
    "    df['has_rain'] = (df['rain_1h'] > 0).astype(int)\n",
    "    df['has_snow'] = (df['snow_1h'] > 0).astype(int)\n",
    "    \n",
    "    # Interactions\n",
    "    df['wind_temp_interaction'] = df['wind_speed_avg'] * df['outdoor_temp']\n",
    "    df['wind_pressure_interaction'] = df['wind_speed_avg'] * df['pressure']\n",
    "    df['wind_humidity_interaction'] = df['wind_speed_avg'] * df['humidity']\n",
    "    df['rotor_wind_interaction'] = df['rotor_angular_velocity'] * df['wind_speed_avg']\n",
    "    \n",
    "    # Rolling features (training only)\n",
    "    if is_training:\n",
    "        for window in [6, 12, 24]:\n",
    "            df[f'wind_speed_rolling_mean_{window}'] = df['wind_speed_avg'].rolling(window=window, min_periods=1).mean()\n",
    "            df[f'wind_speed_rolling_std_{window}'] = df['wind_speed_avg'].rolling(window=window, min_periods=1).std()\n",
    "            df[f'power_rolling_mean_{window}'] = df['active_power'].rolling(window=window, min_periods=1).mean() if 'active_power' in df.columns else 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Feature engineering loaded\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering loaded\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "698d7098",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:21:29.515329Z",
     "start_time": "2025-11-21T17:21:28.820360Z"
    }
   },
   "source": [
    "# Apply feature engineering\n",
    "print(\"Engineering features...\")\n",
    "train_eng = engineer_features(train_df, is_training=True)\n",
    "test_eng = engineer_features(test_df, is_training=False)\n",
    "\n",
    "# Prepare data\n",
    "exclude_cols = ['timestamp', 'active_power']\n",
    "train_features = [col for col in train_eng.columns if col not in exclude_cols]\n",
    "test_features = [col for col in test_eng.columns if col != 'timestamp']\n",
    "common_features = sorted(list(set(train_features) & set(test_features)))\n",
    "\n",
    "X = train_eng[common_features].replace([np.inf, -np.inf], np.nan).fillna(train_eng[common_features].median())\n",
    "y = train_eng['active_power']\n",
    "X_test = test_eng[common_features].replace([np.inf, -np.inf], np.nan).fillna(X.median())\n",
    "\n",
    "print(f\"Features: {len(common_features)}, Samples: {len(X)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features...\n",
      "Features: 56, Samples: 208910\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "3d053de9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:21:29.731665Z",
     "start_time": "2025-11-21T17:21:29.521748Z"
    }
   },
   "source": [
    "# Train/val split (chronological)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# Scale\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train: {X_train_scaled.shape}, Val: {X_val_scaled.shape}, Test: {X_test_scaled.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (167128, 56), Val: (41782, 56), Test: (52228, 56)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "1d6f0a65",
   "metadata": {},
   "source": [
    "## 1. Loading Pre-trained Models\n",
    "\n",
    "The best-performing model from the hyperparameter optimization phase (XGBoost) is loaded to serve as the base for the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "id": "b5d32f65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:21:29.930896Z",
     "start_time": "2025-11-21T17:21:29.739205Z"
    }
   },
   "source": [
    "# Load V3 Optuna model and study\n",
    "with open('../models/v3_optuna_model.pkl', 'rb') as f:\n",
    "    v3_model = pickle.load(f)\n",
    "\n",
    "with open('../models/v3_optuna_study.pkl', 'rb') as f:\n",
    "    v3_study = pickle.load(f)\n",
    "\n",
    "print(\"V3 Optuna model loaded\")\n",
    "print(f\"Best params: {v3_study.best_params}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V3 Optuna model loaded\n",
      "Best params: {'n_estimators': 236, 'max_depth': 10, 'learning_rate': 0.05511868916364675, 'subsample': 0.8021000422933029, 'colsample_bytree': 0.8194689987438017, 'min_child_weight': 6, 'gamma': 0.24321621843196442, 'reg_alpha': 0.041198122026787916, 'reg_lambda': 1.4331550532505222}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "16f54def",
   "metadata": {},
   "source": [
    "## 2. Training Diverse Base Models\n",
    "\n",
    "To create an effective ensemble, additional models with different underlying algorithms (LightGBM and CatBoost) are trained. Diversity in base models is key to reducing variance and improving generalization."
   ]
  },
  {
   "cell_type": "code",
   "id": "80e9607a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:21:33.684235Z",
     "start_time": "2025-11-21T17:21:29.940387Z"
    }
   },
   "source": [
    "# Train LightGBM (different algorithm, may capture different patterns)\n",
    "print(\"Training LightGBM...\")\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=8,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "lgb_model.fit(X_train_scaled, y_train)\n",
    "lgb_pred = lgb_model.predict(X_val_scaled)\n",
    "lgb_mae = mean_absolute_error(y_val, lgb_pred)\n",
    "print(f\"LightGBM MAE: {lgb_mae:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM...\n",
      "LightGBM MAE: 6.8319\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "570ab7c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:21:39.784378Z",
     "start_time": "2025-11-21T17:21:35.515964Z"
    }
   },
   "source": [
    "# Train CatBoost (handles categorical features well)\n",
    "print(\"Training CatBoost...\")\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=200,\n",
    "    learning_rate=0.1,\n",
    "    depth=8,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "cat_model.fit(X_train_scaled, y_train)\n",
    "cat_pred = cat_model.predict(X_val_scaled)\n",
    "cat_mae = mean_absolute_error(y_val, cat_pred)\n",
    "print(f\"CatBoost MAE: {cat_mae:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CatBoost...\n",
      "CatBoost MAE: 8.3837\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "dd0d198b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:21:39.863761Z",
     "start_time": "2025-11-21T17:21:39.807863Z"
    }
   },
   "source": [
    "# Evaluate V3 Optuna model on current split\n",
    "v3_pred = v3_model.predict(X_val_scaled)\n",
    "v3_mae = mean_absolute_error(y_val, v3_pred)\n",
    "print(f\"\\nV3 Optuna MAE: {v3_mae:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "V3 Optuna MAE: 5.9447\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "26987bae",
   "metadata": {},
   "source": [
    "## 3. Weighted Ensemble Strategy\n",
    "\n",
    "This section explores various weighted averaging strategies to combine predictions from the base models. The weights are determined based on the validation performance of each model."
   ]
  },
  {
   "cell_type": "code",
   "id": "1c04abda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:21:39.884813Z",
     "start_time": "2025-11-21T17:21:39.874693Z"
    }
   },
   "source": [
    "# Try different ensemble combinations\n",
    "print(\"\\nTesting ensemble combinations...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_mae = float('inf')\n",
    "best_weights = None\n",
    "best_combo = None\n",
    "\n",
    "# Simple averaging\n",
    "simple_avg = (v3_pred + lgb_pred + cat_pred) / 3\n",
    "simple_mae = mean_absolute_error(y_val, simple_avg)\n",
    "print(f\"Simple average (equal weights): {simple_mae:.4f}\")\n",
    "if simple_mae < best_mae:\n",
    "    best_mae = simple_mae\n",
    "    best_weights = [1/3, 1/3, 1/3]\n",
    "    best_combo = \"Simple average\"\n",
    "\n",
    "# Weighted by inverse MAE (better models get more weight)\n",
    "weights = np.array([1/v3_mae, 1/lgb_mae, 1/cat_mae])\n",
    "weights = weights / weights.sum()\n",
    "weighted_pred = weights[0]*v3_pred + weights[1]*lgb_pred + weights[2]*cat_pred\n",
    "weighted_mae = mean_absolute_error(y_val, weighted_pred)\n",
    "print(f\"Inverse MAE weighted [{weights[0]:.3f}, {weights[1]:.3f}, {weights[2]:.3f}]: {weighted_mae:.4f}\")\n",
    "if weighted_mae < best_mae:\n",
    "    best_mae = weighted_mae\n",
    "    best_weights = weights\n",
    "    best_combo = \"Inverse MAE weighted\"\n",
    "\n",
    "# Heavy weight on best model (V3 Optuna)\n",
    "heavy_v3 = 0.7*v3_pred + 0.15*lgb_pred + 0.15*cat_pred\n",
    "heavy_mae = mean_absolute_error(y_val, heavy_v3)\n",
    "print(f\"Heavy V3 [0.7, 0.15, 0.15]: {heavy_mae:.4f}\")\n",
    "if heavy_mae < best_mae:\n",
    "    best_mae = heavy_mae\n",
    "    best_weights = [0.7, 0.15, 0.15]\n",
    "    best_combo = \"Heavy V3\"\n",
    "\n",
    "# Best two models only (V3 + better of LGB/Cat)\n",
    "if lgb_mae < cat_mae:\n",
    "    two_model = 0.75*v3_pred + 0.25*lgb_pred\n",
    "    two_mae = mean_absolute_error(y_val, two_model)\n",
    "    print(f\"V3 + LightGBM [0.75, 0.25]: {two_mae:.4f}\")\n",
    "    if two_mae < best_mae:\n",
    "        best_mae = two_mae\n",
    "        best_weights = [0.75, 0.25, 0]\n",
    "        best_combo = \"V3 + LightGBM\"\n",
    "else:\n",
    "    two_model = 0.75*v3_pred + 0.25*cat_pred\n",
    "    two_mae = mean_absolute_error(y_val, two_model)\n",
    "    print(f\"V3 + CatBoost [0.75, 0, 0.25]: {two_mae:.4f}\")\n",
    "    if two_mae < best_mae:\n",
    "        best_mae = two_mae\n",
    "        best_weights = [0.75, 0, 0.25]\n",
    "        best_combo = \"V3 + CatBoost\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"BEST ENSEMBLE: {best_combo}\")\n",
    "print(f\"Weights [V3, LGB, Cat]: {best_weights}\")\n",
    "print(f\"Validation MAE: {best_mae:.4f}\")\n",
    "print(f\"\\nImprovement over V3 alone: {((v3_mae - best_mae) / v3_mae * 100):.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing ensemble combinations...\n",
      "============================================================\n",
      "Simple average (equal weights): 6.4843\n",
      "Inverse MAE weighted [0.388, 0.337, 0.275]: 6.3560\n",
      "Heavy V3 [0.7, 0.15, 0.15]: 6.0207\n",
      "V3 + LightGBM [0.75, 0.25]: 5.9594\n",
      "\n",
      "============================================================\n",
      "BEST ENSEMBLE: V3 + LightGBM\n",
      "Weights [V3, LGB, Cat]: [0.75, 0.25, 0]\n",
      "Validation MAE: 5.9594\n",
      "\n",
      "Improvement over V3 alone: -0.25%\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "e953e4d4",
   "metadata": {},
   "source": [
    "## 4. Stacking Meta-Learner\n",
    "\n",
    "Stacking involves training a meta-learner (in this case, Ridge Regression) to combine the predictions of the base models. This allows the model to learn the optimal combination of base model outputs."
   ]
  },
  {
   "cell_type": "code",
   "id": "1018660c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:21:39.915429Z",
     "start_time": "2025-11-21T17:21:39.907632Z"
    }
   },
   "source": [
    "# Create meta-features from base models\n",
    "print(\"\\nTraining stacking meta-learner...\")\n",
    "meta_features_val = np.column_stack([v3_pred, lgb_pred, cat_pred])\n",
    "\n",
    "# Train simple Ridge regression as meta-learner\n",
    "meta_learner = Ridge(alpha=1.0)\n",
    "meta_learner.fit(meta_features_val, y_val)\n",
    "stacking_pred = meta_learner.predict(meta_features_val)\n",
    "stacking_mae = mean_absolute_error(y_val, stacking_pred)\n",
    "\n",
    "print(f\"Stacking MAE: {stacking_mae:.4f}\")\n",
    "print(f\"Meta-learner weights: {meta_learner.coef_}\")\n",
    "\n",
    "if stacking_mae < best_mae:\n",
    "    print(\"Stacking is the best approach.\")\n",
    "    best_mae = stacking_mae\n",
    "    best_combo = \"Stacking\"\n",
    "else:\n",
    "    print(f\"Stacking ({stacking_mae:.4f}) did not outperform weighted ensemble ({best_mae:.4f})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training stacking meta-learner...\n",
      "Stacking MAE: 6.0897\n",
      "Meta-learner weights: [0.68429567 0.2236256  0.09045001]\n",
      "Stacking (6.0897) did not outperform weighted ensemble (5.9594)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "2a07b93c",
   "metadata": {},
   "source": [
    "## 5. Final Prediction Generation\n",
    "\n",
    "The best-performing ensemble method is selected to generate predictions for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "00f5235e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:21:40.124311Z",
     "start_time": "2025-11-21T17:21:39.938645Z"
    }
   },
   "source": [
    "# Generate predictions on test set from all models\n",
    "print(\"\\nGenerating test predictions...\")\n",
    "v3_test = v3_model.predict(X_test_scaled)\n",
    "lgb_test = lgb_model.predict(X_test_scaled)\n",
    "cat_test = cat_model.predict(X_test_scaled)\n",
    "\n",
    "# Apply best ensemble method\n",
    "if best_combo == \"Stacking\":\n",
    "    meta_features_test = np.column_stack([v3_test, lgb_test, cat_test])\n",
    "    final_predictions = meta_learner.predict(meta_features_test)\n",
    "else:\n",
    "    final_predictions = best_weights[0]*v3_test + best_weights[1]*lgb_test + best_weights[2]*cat_test\n",
    "\n",
    "# Apply physical constraints\n",
    "final_predictions = np.clip(final_predictions, 0, 2100)\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(final_predictions)),\n",
    "    'active_power': final_predictions\n",
    "})\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "filename = f'../results/v4_ensemble_submission_{timestamp}.csv'\n",
    "submission.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"\\nSubmission saved: {filename}\")\n",
    "print(f\"Method: {best_combo}\")\n",
    "print(f\"Predictions - Min: {submission['active_power'].min():.2f}, Max: {submission['active_power'].max():.2f}, Mean: {submission['active_power'].mean():.2f}\")\n",
    "print(f\"\\nValidation MAE: {best_mae:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating test predictions...\n",
      "\n",
      "Submission saved: ../results/v4_ensemble_submission_20251121-182140.csv\n",
      "Method: V3 + LightGBM\n",
      "Predictions - Min: 0.00, Max: 2073.74, Mean: 356.32\n",
      "\n",
      "Validation MAE: 5.9594\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "7607801f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:21:40.145373Z",
     "start_time": "2025-11-21T17:21:40.134649Z"
    }
   },
   "source": [
    "# Save models\n",
    "with open('../models/v4_lgb_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lgb_model, f)\n",
    "\n",
    "with open('../models/v4_cat_model.pkl', 'wb') as f:\n",
    "    pickle.dump(cat_model, f)\n",
    "\n",
    "if best_combo == \"Stacking\":\n",
    "    with open('../models/v4_meta_learner.pkl', 'wb') as f:\n",
    "        pickle.dump(meta_learner, f)\n",
    "\n",
    "ensemble_info = {\n",
    "    'method': best_combo,\n",
    "    'weights': best_weights,\n",
    "    'val_mae': best_mae\n",
    "}\n",
    "with open('../models/v4_ensemble_info.pkl', 'wb') as f:\n",
    "    pickle.dump(ensemble_info, f)\n",
    "\n",
    "print(\"Models and ensemble info saved\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and ensemble info saved\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "531f4cea",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "The ensemble approach has demonstrated the potential to improve predictive performance by combining the strengths of multiple models (XGBoost, LightGBM, and CatBoost). The best performing strategy (Weighted Ensemble) was selected for the final submission.\n",
    "\n",
    "### Next Step\n",
    "\n",
    "In the final notebook (`04_neural_network_approach.ipynb`), we will implement a Neural Network (Multi-Layer Perceptron) to compare deep learning performance against our optimized gradient boosting ensemble. This will help us understand if non-tree-based architectures can capture patterns that gradient boosting might miss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
